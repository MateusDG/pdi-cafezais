# ğŸ“Š Benchmark - AvaliaÃ§Ã£o e ComparaÃ§Ã£o de Algoritmos

**Sistema completo para avaliar performance dos algoritmos de ML.**

---

## ğŸ“ Arquivos

| Script | PropÃ³sito | Status | Tempo |
|--------|-----------|--------|-------|
| `ml_benchmark_suite.py` | **Benchmark completo** | âœ… Funcional | 5-15 min |

---

## ğŸ† ml_benchmark_suite.py

**Sistema Profissional de Benchmark**

### **O que faz:**
- **Compara 4 algoritmos**: SVM, Random Forest, k-NN, Naive Bayes
- **MÃºltiplos datasets**: SintÃ©tico, real, hÃ­brido
- **MÃ©tricas completas**: Accuracy, Precision, Recall, F1-score
- **Cross-validation** 5-fold
- **Matriz de confusÃ£o** detalhada
- **Tempo de execuÃ§Ã£o** por algoritmo
- **RelatÃ³rios JSON** e visuais

### **Como usar:**
```bash
cd ml_system/benchmark/
python ml_benchmark_suite.py
```

### **SaÃ­da esperada:**
```
ğŸ”¬ BENCHMARK SUITE - ComparaÃ§Ã£o de Algoritmos ML

ğŸ“Š ConfiguraÃ§Ã£o:
- Algoritmos: 4 (SVM, Random Forest, k-NN, Naive Bayes)
- Dataset: SintÃ©tico balanceado (300 amostras)
- Cross-validation: 5-fold
- MÃ©tricas: Accuracy, Precision, Recall, F1

ğŸ¤– Testando SVM...
â±ï¸  Tempo: 2.34s | Accuracy: 0.987 Â± 0.012

ğŸŒ² Testando Random Forest...
â±ï¸  Tempo: 1.87s | Accuracy: 1.000 Â± 0.000

ğŸ¯ Testando k-NN...
â±ï¸  Tempo: 0.98s | Accuracy: 0.973 Â± 0.018

ğŸ“ˆ Testando Naive Bayes...
â±ï¸  Tempo: 0.45s | Accuracy: 0.891 Â± 0.025

ğŸ† RANKING FINAL:
1. Random Forest  - 100.0% (Â±0.0%)
2. SVM           - 98.7% (Â±1.2%)
3. k-NN          - 97.3% (Â±1.8%)
4. Naive Bayes   - 89.1% (Â±2.5%)

ğŸ“„ RelatÃ³rios salvos em: ../data/benchmark_results/
```

---

## ğŸ“ˆ MÃ©tricas Avaliadas

### **Principais:**
- **Accuracy**: PrecisÃ£o geral
- **Precision**: PrecisÃ£o por classe
- **Recall**: Cobertura por classe
- **F1-Score**: MÃ©dia harmÃ´nica
- **Tempo de execuÃ§Ã£o**
- **Desvio padrÃ£o** (robustez)

### **Matrizes de ConfusÃ£o:**
```
Random Forest - Confusion Matrix:
           weed  coffee  soil
weed        100      0     0
coffee        0    100     0
soil          0      0   100
```

### **Feature Importance:**
- **Top 5 caracterÃ­sticas** mais importantes
- **AnÃ¡lise por algoritmo** (quando disponÃ­vel)
- **ContribuiÃ§Ã£o relativa** em %

---

## ğŸ¨ Recursos Visuais

### **GrÃ¡ficos gerados:**
- **Barplot**: ComparaÃ§Ã£o de accuracy
- **Boxplot**: DistribuiÃ§Ã£o de scores
- **Confusion Matrix**: Heatmap por algoritmo
- **Feature Importance**: Top caracterÃ­sticas

### **Formato de saÃ­da:**
- **PNG**: GrÃ¡ficos de alta qualidade
- **JSON**: Dados brutos estruturados
- **CSV**: Resultados tabulares

---

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### **ParÃ¢metros personalizÃ¡veis:**
```python
ALGORITHMS = ['svm', 'random_forest', 'knn', 'naive_bayes']
CV_FOLDS = 5              # Cross-validation folds
N_SAMPLES = 300           # Amostras por dataset
TEST_SIZE = 0.3           # ProporÃ§Ã£o de teste
RANDOM_STATE = 42         # Reproducibilidade
```

### **Datasets testados:**
1. **SintÃ©tico balanceado** (300 amostras)
2. **Imagens baixadas** (se disponÃ­vel)
3. **HÃ­brido** (sintÃ©tico + real)

---

## ğŸ“Š AnÃ¡lise de Resultados

### **InterpretaÃ§Ã£o das mÃ©tricas:**

**Accuracy > 95%**: âœ… Excelente
**Accuracy 90-95%**: âœ… Bom
**Accuracy 85-90%**: âš ï¸ AceitÃ¡vel
**Accuracy < 85%**: âŒ Precisa melhorar

### **Tempo de execuÃ§Ã£o:**
- **< 1s**: Muito rÃ¡pido
- **1-5s**: RÃ¡pido
- **5-15s**: Moderado
- **> 15s**: Lento

### **Robustez (desvio padrÃ£o):**
- **< 2%**: Muito estÃ¡vel
- **2-5%**: EstÃ¡vel
- **> 5%**: InstÃ¡vel

---

## ğŸ’¡ Como Usar

### **1. Benchmark bÃ¡sico:**
```bash
python ml_benchmark_suite.py
```

### **2. Com dados reais (apÃ³s download):**
```bash
# 1. Certificar que tem imagens
cd ../download/ && python download_training_images.py

# 2. Executar benchmark
cd ../benchmark/ && python ml_benchmark_suite.py
```

### **3. Analisar resultados:**
```bash
# Ver relatÃ³rios gerados
ls ../data/benchmark_results/
```

---

## ğŸ“ Estrutura de SaÃ­da

```
../data/benchmark_results/
â”œâ”€â”€ benchmark_report_YYYYMMDD_HHMMSS.json
â”œâ”€â”€ confusion_matrices.png
â”œâ”€â”€ algorithm_comparison.png
â”œâ”€â”€ feature_importance.png
â””â”€â”€ detailed_metrics.csv
```

### **ConteÃºdo dos arquivos:**
- **JSON**: Dados completos estruturados
- **PNG**: VisualizaÃ§Ãµes profissionais
- **CSV**: Tabela para anÃ¡lise externa

---

## ğŸ” AnÃ¡lise por Algoritmo

### **Random Forest:**
- âœ… **Melhor accuracy** (tipicamente 100%)
- âœ… **Feature importance** disponÃ­vel
- âœ… **Robusto** a overfitting
- âš ï¸ **Tempo moderado** de treinamento

### **SVM:**
- âœ… **Alta accuracy** (95-99%)
- âœ… **Bom** com dados complexos
- âš ï¸ **SensÃ­vel** a hiperparÃ¢metros
- âŒ **Mais lento** para treinar

### **k-NN:**
- âœ… **Muito rÃ¡pido** para treinar
- âœ… **Simples** e interpretÃ¡vel
- âš ï¸ **Lento** para prediÃ§Ã£o
- âŒ **SensÃ­vel** a ruÃ­do

### **Naive Bayes:**
- âœ… **Extremamente rÃ¡pido**
- âœ… **Boa baseline**
- âš ï¸ **Assume independÃªncia** das features
- âŒ **Menor accuracy** geralmente

---

## ğŸ› Troubleshooting

**Problema:** `ModuleNotFoundError: sklearn`
**SoluÃ§Ã£o:** `pip install scikit-learn matplotlib seaborn`

**Problema:** Pouca memÃ³ria
**SoluÃ§Ã£o:** Reduza `N_SAMPLES` no cÃ³digo

**Problema:** Benchmark muito lento
**SoluÃ§Ã£o:** Reduza `CV_FOLDS` ou use menos algoritmos

**Problema:** GrÃ¡ficos nÃ£o aparecem
**SoluÃ§Ã£o:** Instale `pip install matplotlib seaborn`

---

## ğŸ¯ RecomendaÃ§Ãµes

### **Para produÃ§Ã£o:**
Use **Random Forest** (melhor accuracy + estabilidade)

### **Para velocidade:**
Use **k-NN** (mais rÃ¡pido para treinar)

### **Para interpretabilidade:**
Use **Naive Bayes** (mais simples)

### **Para robustez:**
Use **SVM** (bom com dados ruidosos)

---

**Sistema completo para escolher o melhor algoritmo para seu caso de uso.**